# assignment

Performance Comparison:
- Inference Time: 4.2s → 1.7s (2.5x faster)
- GPU Memory: 8GB → 5GB (-37%)

Future Ideas:
- Quantize model to INT8 for further speedup.
- Implement ONNX Runtime for cross-platform deployment.
